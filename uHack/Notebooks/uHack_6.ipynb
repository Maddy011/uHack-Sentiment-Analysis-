{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uHack_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w--SQdCtaVJt",
        "outputId": "f71a056f-70e4-48d2-cf02-f5fb05cfac85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sqlalchemy import create_engine\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/drive/MyDrive/All/Participants_Data_DCW.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6qMqe8SafpR",
        "outputId": "462bf97b-0828-4b77-844d-b3659e20cd73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/All/Participants_Data_DCW.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: __MACOSX/._test.csv     \n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: submission.csv          \n",
            "  inflating: __MACOSX/._submission.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "t3oT6_AiapKA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "BRW4Zrp4aqCA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    \n",
        "    \n",
        "    text = re.sub(r'@[A-Za-z0-9_]+','', text)\n",
        "    \n",
        "    # Dealing with URL links\n",
        "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    text = re.sub(url_regex,'urlplaceholder', text)\n",
        "    # A lot of url are write as follows: http bit.ly. Apply Regex for these cases\n",
        "    utl_regex_2 = 'http [a-zA-Z]+\\.[a-zA-Z]+'\n",
        "    text = re.sub(utl_regex_2,'urlplaceholder', text)\n",
        "    # Other formats: http : //t.co/ihW64e8Z\n",
        "    utl_regex_3 = 'http \\: //[a-zA-Z]\\.(co|com|pt|ly)/[A-Za-z0-9_]+'\n",
        "    text = re.sub(utl_regex_3,'urlplaceholder', text)\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "    # Contractions\n",
        "    text = re.sub(r\"what's\", 'what is ', text)\n",
        "    text = re.sub(r\"can't\", 'cannot', text)\n",
        "    text = re.sub(r\"\\'s\",' ', text)\n",
        "    text = re.sub(r\"\\'ve\", ' have ', text)\n",
        "    text = re.sub(r\"n't\", ' not ', text)\n",
        "    text = re.sub(r\"im\", 'i am ', text)\n",
        "    text = re.sub(r\"i'm\", 'i am ', text)\n",
        "    text = re.sub(r\"\\'re\", ' are ', text)\n",
        "    text = re.sub(r\"\\'d\", ' would ', text)\n",
        "    text = re.sub(r\"\\'ll\", ' will ', text)\n",
        "                  \n",
        "    # Operations and special words  \n",
        "    text = re.sub('#',' ', text)         \n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub('foof', 'food', text)\n",
        "    text = re.sub('msg', 'message', text)\n",
        "    text = re.sub(' u ', 'you', text)\n",
        "    \n",
        "    # Ponctuation Removal\n",
        "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "    \n",
        "    text = text.split()\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    text = [tok for tok in text if tok not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(w) for w in text]\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "I-bEwVPLa7KG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Text\n",
        "df['Review'] = df['Review'].map(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "FlF9tkzJbMJ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 20000\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(df['Review'])\n",
        "sequences = tokenizer.texts_to_sequences(df['Review'])"
      ],
      "metadata": {
        "id": "oStlmStIcLTd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAXLEN = 50\n",
        "X = pad_sequences(sequences, maxlen=MAXLEN)\n",
        "type(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z353wkjAlA6B",
        "outputId": "3236cabc-1b92-4152-d428-1b2bf1652848"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(df[[ 'Components', 'Delivery and Customer Support',\n",
        "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
        "       'Installation', 'Material', 'Price', 'Quality', 'Usability','Polarity']])\n",
        "\n"
      ],
      "metadata": {
        "id": "LyWWBMS6bXlK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(n_inputs, n_outputs):\n",
        "  #batch_size = 256 # It is the sample size of inputs to be processed at each training stage. \n",
        "  hidden_units = 128\n",
        "  dropout = 0.45\n",
        "\n",
        "  # Nossa  MLP com ReLU e Dropout \n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(hidden_units, input_dim=n_inputs,activation='relu',\n",
        "          kernel_initializer='he_uniform'))\n",
        "\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "  model.add(Dense(64,activation='relu',\n",
        "          kernel_initializer='he_uniform'))\n",
        "\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "  model.add(Dense(n_outputs))\n",
        " \n",
        "  model.add(Activation('sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "TsVqLc4SgM18"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(X,y):\n",
        "  results_test = []\n",
        "  results_train =[]\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta = 0.01)\n",
        "  n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "  cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
        "  for train_ix,test_ix in cv.split(X):\n",
        "    X_train, X_test = X[train_ix], X[test_ix]\n",
        "    y_train,y_test = y[train_ix],y[test_ix]\n",
        "    \n",
        "    model = get_model(n_inputs, n_outputs)\n",
        "    \n",
        "    model.fit(X_train,y_train,verbose = 0,epochs = 10,callbacks = callback)\n",
        "    yhat_train = model.predict(X_train)\n",
        "    yhat_test = model.predict(X_test)\n",
        "\n",
        "    train_log_loss = log_loss(y_train, yhat_train)\n",
        "    test_log_loss = log_loss(y_test,yhat_test)\n",
        "    \n",
        "    results_train.append(train_log_loss)\n",
        "    results_test.append(test_log_loss)\n",
        "\n",
        "  return results_train,results_test,model"
      ],
      "metadata": {
        "id": "abXWX4q0gNzl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_train,results_test,model = evaluate_model(X, y)\n",
        "print(results_train)\n",
        "print(results_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbXaB00rldI7",
        "outputId": "d83cbdfe-10d8-472e-e499-a5960b9eda20"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.144696243089102, 5.149760378089449, 5.139147598550625, 5.110021567655498, 5.146516986717928, 5.127102225071644, 5.142122869434267, 5.163400289846744, 5.119932183254807, 5.122363761489322, 5.136313622038639, 5.11975596975372, 5.126446141951875, 5.147655272294412, 5.11091937152352]\n",
            "[5.092643824578885, 5.123978739748568, 5.172958860964725, 5.225769836822951, 5.0794410487939015, 5.141091233355991, 5.209477201067731, 5.002316869169887, 5.224234875665026, 5.104485185727319, 5.0802717498148695, 5.105678619363003, 5.1436704897550225, 5.068871888088422, 5.2465949227582565]\n"
          ]
        }
      ]
    }
  ]
}