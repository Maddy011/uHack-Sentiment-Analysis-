{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U79z12wWgIql",
    "outputId": "6bf5e507-69ac-4483-fc00-911f6dd03301"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\madha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "XNnCtK8iksDb",
    "outputId": "60670ae4-48e2-4024-e3e8-c594bdbb1c35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\n\\nfrom keras.models import Sequential\\nfrom keras.models import Model\\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\\nfrom keras.layers.embeddings import Embedding\\n\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import Dense\\nfrom keras.layers.recurrent import LSTM\\nfrom keras.layers.wrappers import TimeDistributed\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\\nfrom tensorflow.keras.initializers import RandomNormal\\n\\nfrom sklearn.model_selection import RepeatedKFold\\nfrom sklearn.metrics import log_loss\\nfrom keras.callbacks import ModelCheckpoint\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvLvAfnNhLZp",
    "outputId": "454420ce-689e-46d7-97bd-c8868962e3c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/All/Participants_Data_DCW.zip\n",
      "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tfanFqWUhVSR"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PBTF4GJuhVxk"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Dwe3TzHihXpr"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'@[A-Za-z0-9_]+','', text)\n",
    "    \n",
    "    # Dealing with URL links\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    text = re.sub(url_regex,'urlplaceholder', text)\n",
    "    # A lot of url are write as follows: http bit.ly. Apply Regex for these cases\n",
    "    utl_regex_2 = 'http [a-zA-Z]+\\.[a-zA-Z]+'\n",
    "    text = re.sub(utl_regex_2,'urlplaceholder', text)\n",
    "    # Other formats: http : //t.co/ihW64e8Z\n",
    "    utl_regex_3 = 'http \\: //[a-zA-Z]\\.(co|com|pt|ly)/[A-Za-z0-9_]+'\n",
    "    text = re.sub(utl_regex_3,'urlplaceholder', text)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # Contractions\n",
    "    text = re.sub(r\"what's\", 'what is ', text)\n",
    "    text = re.sub(r\"can't\", 'cannot', text)\n",
    "    text = re.sub(r\"\\'s\",' ', text)\n",
    "    text = re.sub(r\"\\'ve\", ' have ', text)\n",
    "    text = re.sub(r\"n't\", ' not ', text)\n",
    "    text = re.sub(r\"im\", 'i am ', text)\n",
    "    text = re.sub(r\"i'm\", 'i am ', text)\n",
    "    text = re.sub(r\"\\'re\", ' are ', text)\n",
    "    text = re.sub(r\"\\'d\", ' would ', text)\n",
    "    text = re.sub(r\"\\'ll\", ' will ', text)\n",
    "                  \n",
    "    # Operations and special words  \n",
    "    text = re.sub('#',' ', text)         \n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub('foof', 'food', text)\n",
    "    text = re.sub('msg', 'message', text)\n",
    "    text = re.sub(' u ', 'you', text)\n",
    "    \n",
    "    # Ponctuation Removal\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    text = [tok for tok in text if tok not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(w) for w in text]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KYkSyw71hpI1"
   },
   "outputs": [],
   "source": [
    "# Cleaning Text\n",
    "df['Review'] = df['Review'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MYjQubUhrZC",
    "outputId": "496cf679-0259-40f0-ed71-5f219a02f7ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autokeras"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\madha\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached autokeras-1.0.16.post1-py3-none-any.whl (166 kB)\n",
      "Requirement already satisfied: scikit-learn in a:\\madhana\\program\\anaconda\\lib\\site-packages (from autokeras) (0.23.2)\n",
      "Collecting keras-tuner<1.1,>=1.0.2\n",
      "  Using cached keras_tuner-1.0.4-py3-none-any.whl (97 kB)\n",
      "Collecting tensorflow<2.6,>=2.3.0\n",
      "  Using cached tensorflow-2.5.2-cp38-cp38-win_amd64.whl (422.7 MB)\n",
      "Requirement already satisfied: packaging in a:\\madhana\\program\\anaconda\\lib\\site-packages (from autokeras) (20.4)\n",
      "Requirement already satisfied: pandas in a:\\madhana\\program\\anaconda\\lib\\site-packages (from autokeras) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from scikit-learn->autokeras) (1.19.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from scikit-learn->autokeras) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from scikit-learn->autokeras) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from scikit-learn->autokeras) (1.5.2)\n",
      "Collecting kt-legacy\n",
      "  Using cached kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: ipython in a:\\madhana\\program\\anaconda\\lib\\site-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (7.19.0)\n",
      "Requirement already satisfied: requests in a:\\madhana\\program\\anaconda\\lib\\site-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (2.24.0)\n",
      "Requirement already satisfied: tensorboard in a:\\madhana\\program\\anaconda\\lib\\site-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (2.7.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.1.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: google-pasta~=0.2 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.12.1)\n",
      "Requirement already satisfied: six~=1.15.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.15.0)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Using cached keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (3.19.1)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Using cached grpcio-1.34.1-cp38-cp38-win_amd64.whl (2.9 MB)\n",
      "Requirement already satisfied: absl-py~=0.10 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (0.15.0)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.6.3)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0\n",
      "  Using cached tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (0.35.1)\n",
      "Requirement already satisfied: gast==0.4.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorflow<2.6,>=2.3.0->autokeras) (0.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from packaging->autokeras) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from pandas->autokeras) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from pandas->autokeras) (2.8.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (5.0.5)\n",
      "Requirement already satisfied: jedi>=0.10 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.17.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (3.0.8)\n",
      "Requirement already satisfied: backcall in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.2.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.4.4)\n",
      "Requirement already satisfied: pickleshare in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.7.5)\n",
      "Requirement already satisfied: pygments in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (2.7.2)\n",
      "Requirement already satisfied: decorator in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (4.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (50.3.1.post20201107)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from requests->keras-tuner<1.1,>=1.0.2->autokeras) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from requests->keras-tuner<1.1,>=1.0.2->autokeras) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from requests->keras-tuner<1.1,>=1.0.2->autokeras) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from requests->keras-tuner<1.1,>=1.0.2->autokeras) (2.10)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (2.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (1.0.1)\n",
      "Requirement already satisfied: ipython-genutils in a:\\madhana\\program\\anaconda\\lib\\site-packages (from traitlets>=4.2->ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.2.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from jedi>=0.10->ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in a:\\madhana\\program\\anaconda\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.2.5)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in a:\\madhana\\program\\anaconda\\lib\\site-packages (from markdown>=2.6.8->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (4.10.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in a:\\madhana\\program\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in a:\\madhana\\program\\anaconda\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (0.4.8)\n",
      "Installing collected packages: kt-legacy, keras-tuner, flatbuffers, keras-nightly, grpcio, h5py, tensorflow-estimator, tensorflow, autokeras\n",
      "Successfully installed autokeras-1.0.16.post1 flatbuffers-1.12 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-tuner-1.0.4 kt-legacy-1.0.4 tensorflow-2.5.2 tensorflow-estimator-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --user autokeras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VlCcdq9Bn4ki"
   },
   "outputs": [],
   "source": [
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Yhwj0HVXiAnh"
   },
   "outputs": [],
   "source": [
    "y = np.array(df[[ 'Components', 'Delivery and Customer Support',\n",
    "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
    "       'Installation', 'Material', 'Price', 'Quality', 'Usability','Polarity']])\n",
    "X = df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "RB39P4rzkX1B",
    "outputId": "5edb6b6b-94a9-441b-8020-c81318935a89"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nvectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,4), max_features=300)\\nvectorizer.fit(X) # fit has to happen only on train data\\nX = vectorizer.transform(X)\\ntype(X)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,4), max_features=300)\n",
    "vectorizer.fit(X) # fit has to happen only on train data\n",
    "X = vectorizer.transform(X)\n",
    "type(X)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rwHVeACFk7LA"
   },
   "outputs": [],
   "source": [
    " Xfeatures = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "t6l0eeb1lvQv"
   },
   "outputs": [],
   "source": [
    "# Split Data \n",
    "X_train,X_test,y_train,y_test = train_test_split(Xfeatures,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_Av_20Alyif",
    "outputId": "fc8fc724-a308-4b34-a790-3a5b7a28917b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [00h 36m 45s]\n",
      "val_loss: 5.977721214294434\n",
      "\n",
      "Best val_loss So Far: 5.945252418518066\n",
      "Total elapsed time: 01h 13m 26s\n",
      "\n",
      "Search: Running Trial #5\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "text_block_1/bl...|bert              |bert              \n",
      "classification_...|0                 |0                 \n",
      "optimizer         |adam              |adam_weight_decay \n",
      "learning_rate     |2e-05             |2e-05             \n",
      "text_block_1/ma...|20000             |20000             \n",
      "text_block_1/be...|512               |512               \n",
      "\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model._config_dict\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model._config_dict.initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model._config_dict.initializer.config\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      " 46/108 [===========>..................] - ETA: 19:34 - loss: 5.3712 - accuracy: 0.0041"
     ]
    }
   ],
   "source": [
    "# Initialize the text classifier.\n",
    "clf = ak.TextClassifier(\n",
    "    overwrite=True, max_trials=15\n",
    ")  \n",
    "# Feed the text classifier with training data.\n",
    "clf.fit(X_train, y_train, epochs=1)\n",
    "# Predict with the best model.\n",
    "predicted_y = clf.predict(x_test)\n",
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSs0anEBuWFo"
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
