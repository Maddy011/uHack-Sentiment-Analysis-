{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uHack_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w--SQdCtaVJt",
        "outputId": "977ea778-4f54-4282-e092-fe67551d843e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sqlalchemy import create_engine\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/drive/MyDrive/All/Participants_Data_DCW.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6qMqe8SafpR",
        "outputId": "476ecbc3-3ddf-472d-e599-8ed63bfd965f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/All/Participants_Data_DCW.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: __MACOSX/._test.csv     \n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: submission.csv          \n",
            "  inflating: __MACOSX/._submission.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "t3oT6_AiapKA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "BRW4Zrp4aqCA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    \n",
        "    \n",
        "    text = re.sub(r'@[A-Za-z0-9_]+','', text)\n",
        "    \n",
        "    # Dealing with URL links\n",
        "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    text = re.sub(url_regex,'urlplaceholder', text)\n",
        "    # A lot of url are write as follows: http bit.ly. Apply Regex for these cases\n",
        "    utl_regex_2 = 'http [a-zA-Z]+\\.[a-zA-Z]+'\n",
        "    text = re.sub(utl_regex_2,'urlplaceholder', text)\n",
        "    # Other formats: http : //t.co/ihW64e8Z\n",
        "    utl_regex_3 = 'http \\: //[a-zA-Z]\\.(co|com|pt|ly)/[A-Za-z0-9_]+'\n",
        "    text = re.sub(utl_regex_3,'urlplaceholder', text)\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "    # Contractions\n",
        "    text = re.sub(r\"what's\", 'what is ', text)\n",
        "    text = re.sub(r\"can't\", 'cannot', text)\n",
        "    text = re.sub(r\"\\'s\",' ', text)\n",
        "    text = re.sub(r\"\\'ve\", ' have ', text)\n",
        "    text = re.sub(r\"n't\", ' not ', text)\n",
        "    text = re.sub(r\"im\", 'i am ', text)\n",
        "    text = re.sub(r\"i'm\", 'i am ', text)\n",
        "    text = re.sub(r\"\\'re\", ' are ', text)\n",
        "    text = re.sub(r\"\\'d\", ' would ', text)\n",
        "    text = re.sub(r\"\\'ll\", ' will ', text)\n",
        "                  \n",
        "    # Operations and special words  \n",
        "    text = re.sub('#',' ', text)         \n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub('foof', 'food', text)\n",
        "    text = re.sub('msg', 'message', text)\n",
        "    text = re.sub(' u ', 'you', text)\n",
        "    \n",
        "    # Ponctuation Removal\n",
        "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "    \n",
        "    text = text.split()\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    text = [tok for tok in text if tok not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(w) for w in text]\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "I-bEwVPLa7KG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Text\n",
        "df['Review'] = df['Review'].map(lambda x: clean_text(x))\n",
        "X = df['Review']"
      ],
      "metadata": {
        "id": "FlF9tkzJbMJ8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,4), max_features=300)\n",
        "vectorizer.fit(X) # fit has to happen only on train data\n",
        "X = vectorizer.transform(X)\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wUDlAFU7pH2",
        "outputId": "3d9d0dd3-dbd3-4afa-f634-0498bca75af1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6136, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.todense()"
      ],
      "metadata": {
        "id": "XCIWH0SO8Ee2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "vocabulary_size = 20000\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(df['Review'])\n",
        "sequences = tokenizer.texts_to_sequences(df['Review'])\n",
        "'''"
      ],
      "metadata": {
        "id": "oStlmStIcLTd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e5b6489-9678-41ff-d58a-f56c9e38ca77"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nvocabulary_size = 20000\\ntokenizer = Tokenizer(num_words=vocabulary_size)\\ntokenizer.fit_on_texts(df['Review'])\\nsequences = tokenizer.texts_to_sequences(df['Review'])\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "MAXLEN = 50\n",
        "X = pad_sequences(sequences, maxlen=MAXLEN)\n",
        "type(X)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z353wkjAlA6B",
        "outputId": "a5ca13b0-d1ad-426f-c6a9-185f08afb193"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMAXLEN = 50\\nX = pad_sequences(sequences, maxlen=MAXLEN)\\ntype(X)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(df[[ 'Components', 'Delivery and Customer Support',\n",
        "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
        "       'Installation', 'Material', 'Price', 'Quality', 'Usability','Polarity']])\n",
        "\n"
      ],
      "metadata": {
        "id": "LyWWBMS6bXlK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(n_inputs, n_outputs):\n",
        "  batch_size = 256 # It is the sample size of inputs to be processed at each training stage. \n",
        "  hidden_units = 128\n",
        "  dropout = 0.45\n",
        "\n",
        "  # Nossa  MLP com ReLU e Dropout \n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(hidden_units, input_dim=n_inputs,activation='relu',\n",
        "          kernel_initializer='he_uniform'))\n",
        "\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "  model.add(Dense(64,activation='relu',\n",
        "          kernel_initializer='he_uniform'))\n",
        "\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "  model.add(Dense(n_outputs))\n",
        " \n",
        "  model.add(Activation('sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "TsVqLc4SgM18"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(X,y):\n",
        "  results_test = []\n",
        "  results_train =[]\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta = 0.01)\n",
        "  n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "  cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
        "  for train_ix,test_ix in cv.split(X):\n",
        "    X_train, X_test = X[train_ix], X[test_ix]\n",
        "    y_train,y_test = y[train_ix],y[test_ix]\n",
        "    \n",
        "    model = get_model(n_inputs, n_outputs)\n",
        "    \n",
        "    model.fit(X_train,y_train,epochs = 10,callbacks = callback)\n",
        "    yhat_train = model.predict(X_train)\n",
        "    yhat_test = model.predict(X_test)\n",
        "\n",
        "    train_log_loss = log_loss(y_train, yhat_train)\n",
        "    test_log_loss = log_loss(y_test,yhat_test)\n",
        "    \n",
        "    results_train.append(train_log_loss)\n",
        "    results_test.append(test_log_loss)\n",
        "\n",
        "  return results_train,results_test,model"
      ],
      "metadata": {
        "id": "abXWX4q0gNzl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_train,results_test,model = evaluate_model(X, y)\n",
        "print(results_train)\n",
        "print(results_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbXaB00rldI7",
        "outputId": "5c13255e-9df5-4d86-8202-733f4d39720a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4507\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3417\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2956\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2687\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2543\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2452\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2350\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2322\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2283\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2214\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4473\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3396\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2971\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2686\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2534\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2425\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2367\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2314\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2260\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2214\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4482\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3428\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2985\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2722\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2561\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2446\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2378\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2330\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2257\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2206\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4664\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3465\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3008\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2745\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2558\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2447\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2381\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2323\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2270\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2264\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4479\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3406\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2951\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2692\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2547\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2420\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2336\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2291\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2219\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2212\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4572\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3469\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2968\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2687\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2544\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2429\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2366\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2301\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2258\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2230\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4590\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3472\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2982\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2701\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2558\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2443\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2382\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2315\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2275\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2222\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4576\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3480\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3007\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2739\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2573\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2470\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2399\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2327\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2272\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2252\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4563\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3469\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2994\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2704\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2523\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2433\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2359\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2292\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2252\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2222\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4407\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3431\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2949\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2670\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2531\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2409\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2354\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2297\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2251\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2234\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4487\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3370\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2936\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2696\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2567\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2448\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2353\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2309\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2269\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2217\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4693\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3520\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3020\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2740\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2608\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2495\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2419\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2362\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2325\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2262\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4642\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3453\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2989\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2690\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2524\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2429\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2365\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2305\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2271\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2208\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4595\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3460\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2973\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2703\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2545\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2442\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2367\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2315\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2281\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2224\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 2ms/step - loss: 0.4512\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.3464\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2963\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2689\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2516\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2427\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2345\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2292\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2258\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 0s 2ms/step - loss: 0.2208\n",
            "[3.4384768738941927, 3.4282492974985153, 3.4192227314703088, 3.4331331706370034, 3.4325626454010445, 3.425138293537019, 3.410715150636431, 3.45962455043071, 3.402198921865542, 3.4316735714461513, 3.426525702923172, 3.4551694058101905, 3.4129995450841895, 3.4462954704121382, 3.39944342730074]\n",
            "[3.618105545154612, 3.6659795307442726, 3.673924243148984, 3.756378941183277, 3.642562236257365, 3.64281344253582, 3.6993904864049676, 3.5426783167256897, 3.786023872600111, 3.663354630186583, 3.639374070427705, 3.629904498632302, 3.664479923374593, 3.646796077902762, 3.756507565019183]\n"
          ]
        }
      ]
    }
  ]
}