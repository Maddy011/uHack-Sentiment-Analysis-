{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_uHack_01.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCKUfzXvsurR",
        "outputId": "75978fba-05f8-4d57-e237-666b970ed9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sqlalchemy import create_engine\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input, Bidirectional, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/drive/MyDrive/All/Participants_Data_DCW.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zLkItA0s3Ye",
        "outputId": "885df1ae-591e-4a8e-98d0-0697196cf1dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/All/Participants_Data_DCW.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: __MACOSX/._test.csv     \n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: submission.csv          \n",
            "  inflating: __MACOSX/._submission.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "nLuIhBImzl_a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Twk66TNUzmoZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    \n",
        "    \n",
        "    text = re.sub(r'@[A-Za-z0-9_]+','', text)\n",
        "    \n",
        "    # Dealing with URL links\n",
        "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    text = re.sub(url_regex,'urlplaceholder', text)\n",
        "    # A lot of url are write as follows: http bit.ly. Apply Regex for these cases\n",
        "    utl_regex_2 = 'http [a-zA-Z]+\\.[a-zA-Z]+'\n",
        "    text = re.sub(utl_regex_2,'urlplaceholder', text)\n",
        "    # Other formats: http : //t.co/ihW64e8Z\n",
        "    utl_regex_3 = 'http \\: //[a-zA-Z]\\.(co|com|pt|ly)/[A-Za-z0-9_]+'\n",
        "    text = re.sub(utl_regex_3,'urlplaceholder', text)\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "    # Contractions\n",
        "    text = re.sub(r\"what's\", 'what is ', text)\n",
        "    text = re.sub(r\"can't\", 'cannot', text)\n",
        "    text = re.sub(r\"\\'s\",' ', text)\n",
        "    text = re.sub(r\"\\'ve\", ' have ', text)\n",
        "    text = re.sub(r\"n't\", ' not ', text)\n",
        "    text = re.sub(r\"im\", 'i am ', text)\n",
        "    text = re.sub(r\"i'm\", 'i am ', text)\n",
        "    text = re.sub(r\"\\'re\", ' are ', text)\n",
        "    text = re.sub(r\"\\'d\", ' would ', text)\n",
        "    text = re.sub(r\"\\'ll\", ' will ', text)\n",
        "                  \n",
        "    # Operations and special words  \n",
        "    text = re.sub('#',' ', text)         \n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub('foof', 'food', text)\n",
        "    text = re.sub('msg', 'message', text)\n",
        "    text = re.sub(' u ', 'you', text)\n",
        "    \n",
        "    # Ponctuation Removal\n",
        "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "    \n",
        "    text = text.split()\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    text = [tok for tok in text if tok not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(w) for w in text]\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "mPlZoBgmzoTs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Text\n",
        "df['Review'] = df['Review'].map(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "eqo28l4CzrWN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# vocabulary_size = 7000\n",
        "tk = Tokenizer(num_words=None,char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(df['Review'])\n",
        "# word_index = tk.word_index\n",
        "# sequences = tk.texts_to_sequences(df['Review'])\n"
      ],
      "metadata": {
        "id": "RYF4ajO9zr6W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1"
      ],
      "metadata": {
        "id": "TW1hTLi_zvCE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string to index\n",
        "train_sequences = tk.texts_to_sequences(df['Review'])\n",
        "# test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_sequences, maxlen=1014, padding='post')\n",
        "# test_data = pad_sequences(test_texts, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "# test_data = np.array(test_data, dtype='float32')\n",
        "X=train_data"
      ],
      "metadata": {
        "id": "_r8fu78mzxBQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(tk.word_index)"
      ],
      "metadata": {
        "id": "YZltyinpzzSe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights=[]\n",
        "embedding_weights.append(np.zeros(vocab_size))\n",
        "\n",
        "for char, i in tk.word_index.items():\n",
        "  onehot=np.zeros(vocab_size)\n",
        "  onehot[i-1]=1\n",
        "  embedding_weights.append(onehot)\n",
        "embedding_weights=np.array(embedding_weights)"
      ],
      "metadata": {
        "id": "xOYrywn_zzJi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfV3Rsm0z3Wl",
        "outputId": "36ae35a2-e70b-4eb0-c095-c3dc448a1d65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 69)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter \n",
        "input_size = 1014\n",
        "# vocab_size = 69\n",
        "embedding_size = 69\n",
        "conv_layers = [[256, 7, 3], \n",
        "               [256, 7, 3], \n",
        "               [256, 3, -1], \n",
        "               [256, 3, -1], \n",
        "               [256, 3, -1], \n",
        "               [256, 3, 3]]\n",
        "\n",
        "fully_connected_layers = [1024, 1024]\n",
        "num_of_classes = 4\n",
        "dropout_p = 0.5\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'"
      ],
      "metadata": {
        "id": "Ci__XHU0z-MG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer Initialization\n",
        "\n",
        "embedding_layer = Embedding(vocab_size+1, \n",
        "                            embedding_size,\n",
        "                            input_length=input_size,\n",
        "                            weights=[embedding_weights])\n",
        "                            "
      ],
      "metadata": {
        "id": "sLznVOULz_yE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(df[[ 'Components', 'Delivery and Customer Support',\n",
        "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
        "       'Installation', 'Material', 'Price', 'Quality', 'Usability','Polarity']])\n"
      ],
      "metadata": {
        "id": "nghC9Fms0CWE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(n_inputs, n_outputs):\n",
        "  #batch_size = 256 # It is the sample size of inputs to be processed at each training stage. \n",
        "  hidden_units = 128\n",
        "  dropout = 0.45\n",
        "  inputs = Input(shape=(n_inputs), name='input', dtype='int64')\n",
        "  # Nossa  MLP com ReLU e Dropout \n",
        "  model = Sequential()\n",
        " \n",
        "  model.add(Embedding(vocab_size+1,embedding_size,input_length=n_inputs,weights=[embedding_weights])) #(embedding_layer(inputs))\n",
        " \n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "  model.add(Bidirectional(LSTM(100)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "UvJLSgK00LiC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_model(n_inputs, n_outputs):\n",
        "  #batch_size = 256 # It is the sample size of inputs to be processed at each training stage. \n",
        "  hidden_units = 128\n",
        "  dropout = 0.45\n",
        "  inputs = Input(shape=(n_inputs), name='input', dtype='int64')\n",
        "  # Nossa  MLP com ReLU e Dropout \n",
        "  model = Sequential()\n",
        " \n",
        "  model.add(Embedding(vocab_size+1,embedding_size,input_length=n_inputs,weights=[embedding_weights])) #(embedding_layer(inputs))\n",
        " \n",
        "  model.add(Dropout(0.45))\n",
        "  \n",
        "  model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "  model.add(Bidirectional(LSTM(100)))\n",
        "  model.add(Dropout(0.45))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "  return model\n",
        "  '''"
      ],
      "metadata": {
        "id": "90faJCdEuR5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(X,y):\n",
        "  results_test = []\n",
        "  results_train =[]\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta = 0.01)\n",
        "  n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "  cv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=1)\n",
        "  for train_ix,test_ix in cv.split(X):\n",
        "    X_train, X_test = X[train_ix], X[test_ix]\n",
        "    y_train,y_test = y[train_ix],y[test_ix]\n",
        "    \n",
        "    model = get_model(n_inputs, n_outputs)\n",
        "    \n",
        "    model.fit(X_train,y_train,epochs = 1,callbacks = callback)\n",
        "    yhat_train = model.predict(X_train)\n",
        "    yhat_test = model.predict(X_test)\n",
        "\n",
        "    train_log_loss = log_loss(y_train, yhat_train)\n",
        "    test_log_loss = log_loss(y_test,yhat_test)\n",
        "    \n",
        "    results_train.append(train_log_loss)\n",
        "    results_test.append(test_log_loss)\n",
        "\n",
        "  return results_train,results_test,model"
      ],
      "metadata": {
        "id": "sidvMbGS0PWE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_train,results_test,model = evaluate_model(X, y)\n",
        "print(results_train)\n",
        "print(results_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7vyw-uU0YD8",
        "outputId": "89b8ee38-572f-4e8d-9538-420134b1bb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 1014, 69)          4830      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1014, 69)          0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 1014, 32)          6656      \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 507, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 507, 200)         106400    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 200)              240800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                12864     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 12)                780       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 372,330\n",
            "Trainable params: 372,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "  9/128 [=>............................] - ETA: 27:46 - loss: 0.6563 - accuracy: 0.1215"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('/content/test.csv')\n"
      ],
      "metadata": {
        "id": "cU_xTgLx0YoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_processed_test_review =  test_data['Review'].map(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "fATcwKDg0bLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['preprocessed_test_review'] = pre_processed_test_review"
      ],
      "metadata": {
        "id": "p-maQPwl0cuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk.fit_on_texts(test_data['Review'])"
      ],
      "metadata": {
        "id": "1uC61d1R0edf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string to index\n",
        "test_sequences = tk.texts_to_sequences(test_data['Review'])\n",
        "# test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding\n",
        "test_data = pad_sequences(test_sequences, maxlen=1014, padding='post')\n",
        "# test_data = pad_sequences(test_texts, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "test_data = np.array(test_data, dtype='float32')\n",
        "# test_data = np.array(test_data, dtype='float32')\n",
        "X_val=test_data"
      ],
      "metadata": {
        "id": "C8jxro5i0gB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_on_test_data = model.predict(X_val)"
      ],
      "metadata": {
        "id": "LPQEhPQ00jl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(prediction_on_test_data, columns = ['Components', 'Delivery and Customer Support',\n",
        "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
        "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
        "       'Polarity'])\n"
      ],
      "metadata": {
        "id": "4kv7S8Yh0lKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('submission_char_cnn.csv', index=False)"
      ],
      "metadata": {
        "id": "ZP0b6yIj0lq3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}