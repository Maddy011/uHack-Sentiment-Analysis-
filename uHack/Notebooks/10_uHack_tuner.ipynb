{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_uHack_tuner.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pyuI0lp_kYf",
        "outputId": "d8f4d213-4923-49ac-e343-ada1079a0039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sqlalchemy import create_engine\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/drive/MyDrive/All/Participants_Data_DCW.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gFf9rPLAFkW",
        "outputId": "0fb92039-7579-4be1-fa73-152bc1aff66e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/All/Participants_Data_DCW.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: __MACOSX/._test.csv     \n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: submission.csv          \n",
            "  inflating: __MACOSX/._submission.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "MnCNKCAPAF9f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ULS-uErLAHyW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    \n",
        "    \n",
        "    text = re.sub(r'@[A-Za-z0-9_]+','', text)\n",
        "    \n",
        "    # Dealing with URL links\n",
        "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    text = re.sub(url_regex,'urlplaceholder', text)\n",
        "    # A lot of url are write as follows: http bit.ly. Apply Regex for these cases\n",
        "    utl_regex_2 = 'http [a-zA-Z]+\\.[a-zA-Z]+'\n",
        "    text = re.sub(utl_regex_2,'urlplaceholder', text)\n",
        "    # Other formats: http : //t.co/ihW64e8Z\n",
        "    utl_regex_3 = 'http \\: //[a-zA-Z]\\.(co|com|pt|ly)/[A-Za-z0-9_]+'\n",
        "    text = re.sub(utl_regex_3,'urlplaceholder', text)\n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "    # Contractions\n",
        "    text = re.sub(r\"what's\", 'what is ', text)\n",
        "    text = re.sub(r\"can't\", 'cannot', text)\n",
        "    text = re.sub(r\"\\'s\",' ', text)\n",
        "    text = re.sub(r\"\\'ve\", ' have ', text)\n",
        "    text = re.sub(r\"n't\", ' not ', text)\n",
        "    text = re.sub(r\"im\", 'i am ', text)\n",
        "    text = re.sub(r\"i'm\", 'i am ', text)\n",
        "    text = re.sub(r\"\\'re\", ' are ', text)\n",
        "    text = re.sub(r\"\\'d\", ' would ', text)\n",
        "    text = re.sub(r\"\\'ll\", ' will ', text)\n",
        "                  \n",
        "    # Operations and special words  \n",
        "    text = re.sub('#',' ', text)         \n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub('foof', 'food', text)\n",
        "    text = re.sub('msg', 'message', text)\n",
        "    text = re.sub(' u ', 'you', text)\n",
        "    \n",
        "    # Ponctuation Removal\n",
        "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "    \n",
        "    text = text.split()\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    text = [tok for tok in text if tok not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(w) for w in text]\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "BNh4AIWFAQJa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Text\n",
        "df['Review'] = df['Review'].map(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "vpnh4NY0ASeO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['Review']"
      ],
      "metadata": {
        "id": "u1_UoVuyAUkk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,5), max_features=300)\n",
        "vectorizer.fit(X) # fit has to happen only on train data\n",
        "X = vectorizer.transform(X)\n",
        "X.shape\n",
        "type(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0yBN6ogAW2o",
        "outputId": "3640d054-ed1d-48fa-ae8f-a3bd004992c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.toarray()\n",
        "type(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjSoV77UAc6d",
        "outputId": "9feedccb-a604-4b67-badd-3bd57dc72000"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(df[[ 'Components', 'Delivery and Customer Support',\n",
        "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
        "       'Installation', 'Material', 'Price', 'Quality', 'Usability','Polarity']])\n",
        "targets = [ 'Components', 'Delivery and Customer Support',\n",
        "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
        "       'Installation', 'Material', 'Price', 'Quality', 'Usability','Polarity']\n",
        "y_label = df[targets].values\n",
        "\n",
        "y_label.shape      \n",
        "type(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hQx7ylrAef2",
        "outputId": "1d7e41f6-f967-4d11-e071-8099b2c5d6fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxGWW1KUAgNx",
        "outputId": "260e59a8-6b47-42c0-edba-e0f2f6e48d57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.7.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.42.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kerastuner as kt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9mbauemFrCg",
        "outputId": "1fe1caee-e924-4e3b-cc38-94ea79454905"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)"
      ],
      "metadata": {
        "id": "mkz-6stQAkI0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  '''\n",
        "  Args:\n",
        "    hp - Keras tuner object\n",
        "  '''\n",
        "  batch_size = 256 # It is the sample size of inputs to be processed at each training stage. \n",
        "  hidden_units = 128\n",
        "  dropout = 0.45\n",
        "  X_in=X.shape[1]\n",
        "  # Initialize the Sequential API and start stacking the layers\n",
        "  model = Sequential()\n",
        "  model.add(Dense(hidden_units, input_dim=X_in,activation='relu',\n",
        "          kernel_initializer='he_uniform'))\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  model.add(Dropout(dropout))\n",
        "  hp_units = hp.Int('units', min_value=280, max_value=290, step=1)\n",
        "  \n",
        "  model.add(Dense(units=hp_units,activation='relu', name='dense_002',\n",
        "          kernel_initializer='he_uniform'))\n",
        "  # Add next layers\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(Dense(12))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "g0w5L0SQAxel"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the tuner\n",
        "tuner = kt.Hyperband(model_builder, # the hypermodel\n",
        "                     objective='val_accuracy', # objective to optimize\n",
        "max_epochs=10,\n",
        "factor=3, # factor which you have seen above \n",
        "directory='dir', # directory to save logs \n",
        "project_name='khyperband03')"
      ],
      "metadata": {
        "id": "utk-0ZF3FAT8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hypertuning settings\n",
        "tuner.search_space_summary() \n",
        "# Output:- \n",
        "\n",
        "# Search space summary\n",
        "# Default search space size: 2\n",
        "# units (Int)\n",
        "# {'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
        "# learning_rate (Choice)\n",
        "# {'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghi6s7Q2Fzul",
        "outputId": "16f0a6a5-edd9-4466-ed08-96642a1d5e68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 280, 'max_value': 290, 'step': 1, 'sampling': None}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "# Perform hypertuning\n",
        "tuner.search(X_train, y_train, epochs=2, validation_split=0.2, callbacks=[stop_early])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEyHFByWGR0U",
        "outputId": "3ea238c1-29b7-4664-fb8f-136b5880b4d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 00m 04s]\n",
            "val_accuracy: 0.004860267508774996\n",
            "\n",
            "Best val_accuracy So Far: 0.5091130137443542\n",
            "Total elapsed time: 00h 01m 18s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_hp=tuner.get_best_hyperparameters()[0]"
      ],
      "metadata": {
        "id": "l11MleVwGaPi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters\n",
        "h_model = tuner.hypermodel.build(best_hp)\n",
        "h_model.summary()\n",
        "h_model.fit(X_train, y_train, epochs=3, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vucaVgInHCL_",
        "outputId": "b46f7e92-ea1c-48da-953f-fe3ad78a3d93"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 128)               38528     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_002 (Dense)           (None, 280)               36120     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 280)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 12)                3372      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 12)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 78,020\n",
            "Trainable params: 78,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "103/103 [==============================] - 1s 4ms/step - loss: 0.3185 - accuracy: 0.1591 - val_loss: 0.2308 - val_accuracy: 0.3985\n",
            "Epoch 2/3\n",
            "103/103 [==============================] - 0s 3ms/step - loss: 0.2342 - accuracy: 0.3230 - val_loss: 0.2200 - val_accuracy: 0.4447\n",
            "Epoch 3/3\n",
            "103/103 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.3653 - val_loss: 0.2152 - val_accuracy: 0.4107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a6e375510>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# h_model.save('tuner_model.h5')"
      ],
      "metadata": {
        "id": "kjD4uEdUJr-7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_eval_dict = h_model.evaluate(X_val, y_val, return_dict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4wHkdCgIgvy",
        "outputId": "25159044-b37d-435b-ba36-4c61e2519116"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.3610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_test = h_model.predict(X_val)\n",
        "\n",
        "model_log_loss=log_loss(y_val,yhat_test)\n",
        "model_log_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GtEG62pIp-p",
        "outputId": "b47600e8-ddea-4013-e35f-e66f68ab452b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.7563976190745096"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "uD3l11uZJOq6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_processed_test_review =  test_data['Review'].map(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "z1rnJz8CLIkm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['preprocessed_test_review'] = pre_processed_test_review"
      ],
      "metadata": {
        "id": "jIa0limlLLzD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vectorised_data = vectorizer.transform(test_data['preprocessed_test_review'] )\n",
        "test_vectorised_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIeKLoHHLOHU",
        "outputId": "cba0f3f1-9e42-4a79-f5a0-5d7b6a1bef65"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2631, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vectorised_data = test_vectorised_data.todense()"
      ],
      "metadata": {
        "id": "3VxLaOBeLR4v"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_on_test_data = h_model.predict(test_vectorised_data)"
      ],
      "metadata": {
        "id": "CuM6i9pQLUUE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(prediction_on_test_data, columns = ['Components', 'Delivery and Customer Support',\n",
        "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
        "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
        "       'Polarity'])\n"
      ],
      "metadata": {
        "id": "KQrqcy2DLXon"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('submission_tuner3.75639.csv', index=False)"
      ],
      "metadata": {
        "id": "e_hcBEIaLbox"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lTawBpx4LhHO"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}